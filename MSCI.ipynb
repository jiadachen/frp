{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSCI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "77lZAdelzbur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHh6DJlYzjc4",
        "colab_type": "code",
        "outputId": "fb792c15-27a8-4a61-9e58-4a9b2864b5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Creating corpus :\n",
        "\n",
        "file_list = glob.glob(os.path.join(os.getcwd(), \"/Users/michaelwehbe/Desktop/MSCI data/10-K_i1&2\", \"*.txt\"))\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for file_path in file_list:\n",
        "    with open(file_path) as f_input:\n",
        "        corpus.append(f_input.read())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wiwNycazjlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing extra stopwords:\n",
        "\n",
        "file_list2 = glob.glob(os.path.join(os.getcwd(), \"/Users/michaelwehbe/Desktop/MSCI data/extra_stop_words\", \"*.txt\"))\n",
        "\n",
        "extra_stop_words_corpus = []\n",
        "\n",
        "for file_path1 in file_list2:\n",
        "    with open(file_path1) as f_input1:\n",
        "        extra_stop_words_corpus.append(f_input1.read())\n",
        "\n",
        "#names_stop_words = extra_stop_words_corpus[0]\n",
        "#geographic_stop_words = extra_stop_words_corpus[1]\n",
        "#generic_stop_words = extra_stop_words_corpus[3]\n",
        "#dates_stop_words = extra_stop_words_corpus[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqPyULfvUlwl",
        "colab_type": "code",
        "outputId": "a25ea5b9-4b58-4979-f2b8-944a2874e33d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "extra_stop_words_corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIbWM8RVlsmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZTz0IQtzjnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data preprocessing:\n",
        "\n",
        "#Let's define some useful functions:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPPzYmMqzjp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get all the text to lowercase:\n",
        "\n",
        "def text_lowercase(text):\n",
        "    return text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzNp4XHBzjsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove numbers \n",
        "def remove_numbers(text): \n",
        "    result = re.sub(r'\\d+', '', text) \n",
        "    return result "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XvdLQpZzjuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ep4Ffizzjw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove whitespace from text\n",
        "\n",
        "def remove_whitespace(text):\n",
        "    return \" \".join(text.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Sf6fp2zjzD",
        "colab_type": "code",
        "outputId": "a4cf8f0e-b875-43b2-acd1-c0a31761110a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Let's clean the extra stop words lists:\n",
        "\n",
        "def clean_extra_stop_words(extra_stop_words):\n",
        "    a = text_lowercase(extra_stop_words)\n",
        "    b = remove_numbers(a)\n",
        "    a = remove_punctuation(b)\n",
        "    b = remove_whitespace(a)\n",
        "    return list(b.split())\n",
        "\n",
        "\n",
        "names_stop_words_clean = clean_extra_stop_words(names_stop_words)\n",
        "geographic_stop_words_clean = clean_extra_stop_words(geographic_stop_words)\n",
        "generic_stop_words_clean = clean_extra_stop_words(generic_stop_words)\n",
        "dates_stop_words_clean = clean_extra_stop_words(dates_stop_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a75205909987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnames_stop_words_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_extra_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mgeographic_stop_words_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_extra_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeographic_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgeneric_stop_words_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_extra_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'names_stop_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFRUa0s1zj1O",
        "colab_type": "code",
        "outputId": "178706e7-d5af-4666-9423-2e3e3ccbd4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "#Dealing with stopwords:\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Creating stopwords full list\n",
        "from collections import Counter\n",
        "intersection1 = Counter(names_stop_words_clean) & Counter(stopwords.words(\"english\"))\n",
        "intersection2 = Counter(geographic_stop_words_clean) & Counter(stopwords.words(\"english\"))\n",
        "intersection3 = Counter(stopwords.words(\"english\")) & Counter(generic_stop_words_clean)\n",
        "intersection4 = Counter(stopwords.words(\"english\")) & Counter(dates_stop_words_clean)\n",
        "\n",
        "names_without_common = list(Counter(names_stop_words_clean) - intersection1)\n",
        "geographic_without_common = list(Counter(geographic_stop_words_clean) - intersection2)\n",
        "generic_without_common = list(Counter(generic_stop_words_clean) - intersection3)\n",
        "dates_without_common = list(Counter(dates_stop_words_clean) - intersection4)\n",
        "\n",
        "full_stop_words_list = stopwords.words(\"english\") + names_without_common + geographic_without_common + generic_without_common + dates_without_common\n",
        "\n",
        "# full_stop_words_list = list(set(stopwords.words(\"english\") + names_without_common + geographic_without_common + generic_without_common + dates_without_common))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(full_stop_words_list)\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    return filtered_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2c6e84ea0007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Creating stopwords full list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mintersection1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_stop_words_clean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mintersection2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeographic_stop_words_clean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mintersection3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_stop_words_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'names_stop_words_clean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQVTjstHzj3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatize the words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "#Get word type for each word: \n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos =get_wordnet_pos(word)) for word in word_tokens]\n",
        "    return ' '.join(lemmas)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwymHps6zj59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing numeric of words:\n",
        "def remove_lastfirst_numeric(txt):\n",
        "    for j, i in enumerate(txt):\n",
        "        if i[-1].isnumeric():\n",
        "            txt[j] = i[:-1]\n",
        "        if i[0].isnumeric():\n",
        "            txt[j] = i[1:]\n",
        "    return txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEypC6gyzj8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define text preprocessing function:\n",
        "\n",
        "def text_preprocessing(text):\n",
        "\n",
        "    a = text_lowercase(text)\n",
        "    b = remove_numbers(a)\n",
        "    a = remove_punctuation(b)\n",
        "    b = remove_whitespace(a)\n",
        "    a = lemmatize_word(b)\n",
        "    b = remove_stopwords(a)\n",
        "    return remove_lastfirst_numeric(b)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fYb86eYzj-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#n_docs = len(corpus)\n",
        "n_docs = 30\n",
        "\n",
        "#Let's now create the preprocessed corpus:\n",
        "clean_corpus = []\n",
        "clean_corpus_tokenized = []\n",
        "for i in range(n_docs):\n",
        "    clean_corpus.append(' '.join(text_preprocessing(corpus[i])))  #We have a list of strings, each string represent one document \n",
        "    clean_corpus_tokenized.append(text_preprocessing(corpus[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQNpWP8jzkBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_id_vectorizer = TfidfVectorizer(stop_words = 'english', strip_accents = 'ascii', min_df = 5, max_df = 0.5)\n",
        "\n",
        "x = tf_id_vectorizer.fit_transform(clean_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XhyqiitSozN",
        "colab_type": "text"
      },
      "source": [
        "## Let's visualize our data through a word cloud: ##\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6hhE7a_Skcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pagX6ruKSkhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(clean_corpus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYr7XV2MSkmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv593Wi-Skq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordcloud.generate(long_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmxp_pVYSk4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wordcloud.to_image()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r1z_uR8eAS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Exploratory data analysis for each document"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyuc5ufeR3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_top_n_gram(corpus, gram, n):\n",
        "    vec = CountVectorizer(ngram_range=(gram, gram)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "for i in range(n_docs):   \n",
        "    f = plt.figure(figsize=(15,3))\n",
        "    ax1 = f.add_subplot(131)\n",
        "    ax2 = f.add_subplot(132)\n",
        "    ax3 = f.add_subplot(133)\n",
        "\n",
        "    wordcloud = WordCloud(background_color=\"white\",max_words=5000, contour_width=3, contour_color='steelblue').\\\n",
        "        generate(clean_corpus[i])\n",
        "    ax1.imshow(wordcloud)\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    common_words = get_top_n_gram([clean_corpus[i]], 2,10)\n",
        "    df2 = pd.DataFrame(common_words, columns = ['Text' , 'count'])\n",
        "    ax2.bar(df2['Text'],df2['count'])\n",
        "    ax2.tick_params(labelrotation=90)\n",
        "\n",
        "    common_words = get_top_n_gram([clean_corpus[i]], 3,10)\n",
        "    df3 = pd.DataFrame(common_words, columns = ['Text' , 'count'])\n",
        "    ax3.bar(df3['Text'],df3['count'])\n",
        "    ax3.tick_params(labelrotation=90)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krepoMztSzdm",
        "colab_type": "text"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUr3KZnDSk9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import make_multilabel_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiCJgdwCSkxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Parameters: \n",
        "\n",
        "n_topics = 10\n",
        "n_words = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evh5jezbSkwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LatentDirichletAllocation(n_components= n_topics,random_state=9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SayJJlHYz7R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda.fit(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ3vwMimz7Ua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function that yields the topics found by LDA:\n",
        "\n",
        "def print_topics(model, tf_id_vectorizer, n_top_words):\n",
        "    words = tf_id_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]for i in topic.argsort()[:-n_top_words - 1:-1]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_ANt2ygz7W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_topics(lda, tf_id_vectorizer, n_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC7_ATJ8TCBk",
        "colab_type": "text"
      },
      "source": [
        "#### LDA performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WKmUg0Bz7Z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda.score(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaajbm4pTHjP",
        "colab_type": "text"
      },
      "source": [
        "#### Grid search hyperparams to optimize LDA: \n",
        "\n",
        "Mainly \"number of topics\" and \"learning decay\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQrZ9wkpz7cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCGAO7kF0Mrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda_opt = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda_opt, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i-_Eo9nTMEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yutNM2NTR2a",
        "colab_type": "text"
      },
      "source": [
        " Now the LDA model to use is \"best_lda_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMnCBbszTwz0",
        "colab_type": "text"
      },
      "source": [
        "## Visualization of LDA results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDr6EZFZTMGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyLDAvis import sklearn as sklearn_lda\n",
        "import pickle \n",
        "import pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glR0aMaHTMJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LDAvis_prepared = sklearn_lda.prepare(best_lda_model, x, tf_id_vectorizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6A0sy9OTML4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.display(LDAvis_prepared)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLdDW0vRTMOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Document - Topic Matrix\n",
        "lda_output = best_lda_model.transform(x)\n",
        "\n",
        "# column names\n",
        "topicnames = [\"Topic\" + str(i) for i in range(model.best_params_['n_components'])]\n",
        "\n",
        "# index names\n",
        "docnames = [\"Doc\" + str(i) for i in range(n_docs)]\n",
        "\n",
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9ZsRCgpT3oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrisbjoWT4W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgAjqGAPT4Z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Styling\n",
        "def color_green(val):\n",
        "    color = 'green' if val > .1 else 'black'\n",
        "    return 'color: {col}'.format(col=color)\n",
        "\n",
        "def make_bold(val):\n",
        "    weight = 700 if val > .1 else 400\n",
        "    return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwNxFxKkT9wC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply Style\n",
        "df_document_topics = df_document_topic.style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2J6G7J1T95Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8e33RHXT995",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}